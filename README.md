# Product Matching System

This project implements an end-to-end image and text product matching system using FastAPI, Triton Inference Server, ONNX models, MongoDB, and Docker.

---

## ğŸ§± Project Structure

```
product-matching-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI application entry point
â”‚   â”œâ”€â”€ inference.py             # Inference logic for image & text
â”‚   â”œâ”€â”€ populate_db.py           # Populates the DB with embeddings
â”‚   â”œâ”€â”€ logs_db.py               # Handles logging
â”‚   â””â”€â”€ metadata_db.py           # Handles product metadata
â”‚
â”œâ”€â”€ model_repo/
â”‚   â”œâ”€â”€ text_model/
â”‚   â”‚   â””â”€â”€ 1/
â”‚   â”‚       â”œâ”€â”€ model.onnx
â”‚   â”‚       â””â”€â”€ config.pbtxt
â”‚   â””â”€â”€ vision_model/
â”‚       â””â”€â”€ 1/
â”‚           â”œâ”€â”€ model.onnx
â”‚           â””â”€â”€ config.pbtxt
â”‚
â”œâ”€â”€ quantize_model.py           # Optional quantization script
â”œâ”€â”€ text_model.plan             # Optional plan file (for TensorRT)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ§ª Requirements

- Docker
- Python 3.11+
- NVIDIA GPU (optional for GPU acceleration)
- MongoDB
- Triton Inference Server

---

## ğŸš€ Setup and Run

1. **Build and Start Services**

```bash
docker compose up --build
```

2. **Verify Containers**

```bash
docker ps
```

3. **Run Population Script**

```bash
docker exec -it fastapi_app python app/populate_db.py
```

This script loads embeddings for all product entries using your ONNX models and stores them in MongoDB.

---

## âš™ï¸ Environment Variables

Set inside `docker-compose.yml`:

```yaml
environment:
  - MONGO_URI=mongodb://mongodb:27017
  - TRITON_URL=http://triton:8000
```

---

## ğŸ”Œ Model Input/Output

### Image Model (`vision_model`)
- Input: `input_image` shape `[1, 3, 224, 224]`, dtype: FP32
- Output: `embedding` shape `[1, 512]`, dtype: FP32

### Text Model (`text_model`)
- Input: `input_ids`, `attention_mask`, shape `[77]`, dtype: INT32
- Output: `embedding`, shape `[1, 512]`, dtype: FP32

---

## âœ… Notes

- Use `.dockerignore` to exclude large or unnecessary local files.
- `.onnx` files are served by Triton.
- `config.pbtxt` must match model's actual input/output signatures.

---

## ğŸ§  Inference Logic

Implemented in `inference.py`. Communicates with Triton at:
```
http://triton:8000/v2/models/{model_name}/infer
```

- Text and image embeddings are returned as NumPy arrays.
- These embeddings are then stored in MongoDB or compared for similarity.

---

## ğŸ“« API (Optional)

If youâ€™re running FastAPI for inference, it will be served at:
```
http://localhost:8003/
```